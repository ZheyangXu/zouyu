# Advanced PPO Configuration for Parkour Training
algorithm:
  name: "ppo"
  description: "Proximal Policy Optimization with advanced features"
  
  # Core learning parameters
  learning_rate: 3.0e-4
  lr_schedule: "adaptive"  # fixed, linear, adaptive, cosine
  lr_decay_steps: 10000
  lr_min: 1.0e-5
  
  # PPO specific parameters
  clip_param: 0.2
  clip_param_decay: 0.99
  value_loss_coef: 1.0
  entropy_coef: 0.01
  entropy_decay: 0.995
  max_grad_norm: 1.0
  
  # Training parameters
  num_learning_iterations: 10000
  num_steps_per_env: 32
  mini_batch_size: 32768  # Total batch size across all environments
  num_mini_batches: 8
  num_epochs: 8
  
  # GAE (Generalized Advantage Estimation) parameters
  gamma: 0.998
  lambda: 0.95
  
  # Network architecture
  actor_hidden_dims: [512, 256, 128]
  critic_hidden_dims: [512, 256, 128]
  activation: "elu"  # relu, elu, tanh, swish
  
  # Normalization
  normalize_observations: true
  normalize_advantages: true
  normalize_returns: false
  
  # Advanced features
  use_recurrent_policy: false
  recurrent_hidden_size: 512
  use_privileged_info: false
  privileged_info_dim: 64

# Experience replay (optional)
experience_replay:
  enabled: false
  buffer_size: 1000000
  replay_ratio: 0.25
  prioritized: true
  alpha: 0.6
  beta: 0.4

# Exploration
exploration:
  # Action noise
  action_noise:
    enabled: true
    initial_std: 0.3
    final_std: 0.1
    decay_steps: 5000
    
  # Curiosity-driven exploration
  curiosity:
    enabled: false
    intrinsic_reward_coef: 0.01
    feature_dim: 64

# Regularization
regularization:
  # L2 regularization
  weight_decay: 1e-4
  
  # Dropout (if enabled)
  dropout:
    enabled: false
    rate: 0.1
    
  # Early stopping
  early_stopping:
    enabled: true
    patience: 1000
    min_delta: 0.001

# Adaptive learning
adaptive_learning:
  # KL divergence target for adaptive learning rate
  kl_target: 0.01
  kl_tolerance: 1.5
  
  # Performance-based adaptation
  performance_adaptation:
    enabled: true
    success_rate_target: 0.7
    adaptation_rate: 0.1

# Checkpointing and resuming
checkpoints:
  save_interval: 200  # iterations
  keep_checkpoints: 5  # number of checkpoints to keep
  auto_resume: true
  resume_path: null   # specific checkpoint to resume from
  
  # Save best model based on performance
  save_best_model: true
  best_model_metric: "success_rate"  # or "episode_reward"

# Evaluation during training
evaluation:
  enabled: true
  interval: 200       # iterations
  num_episodes: 50
  deterministic: true  # use deterministic policy for evaluation
  
  # Evaluation environments (can be different from training)
  eval_env_config: null  # if null, use same as training
  
  # Video recording during evaluation
  record_video: true
  video_length: 1000  # frames

# Logging and monitoring
logging:
  log_interval: 10    # iterations
  video_interval: 500 # iterations
  
  # Detailed metrics tracking
  metrics:
    # Core RL metrics
    - "episode_reward"
    - "episode_length"
    - "success_rate"
    - "policy_loss"
    - "value_loss"
    - "entropy_loss"
    - "explained_variance"
    - "clipfrac"
    - "approx_kl"
    - "learning_rate"
    
    # Parkour specific metrics
    - "forward_distance"
    - "energy_efficiency"
    - "stability_score"
    - "obstacle_success_rate"
    - "average_velocity"
    - "gait_quality"
    
    # Training diagnostics
    - "gradient_norm"
    - "parameter_norm"
    - "value_function_error"
    - "policy_entropy"
    
  # Advanced logging
  log_gradients: false
  log_weights: false
  log_activations: false

# Hyperparameter optimization
hyperopt:
  enabled: false
  backend: "optuna"  # optuna, ray, hyperopt
  trials: 100
  
  # Parameters to optimize
  search_space:
    learning_rate:
      type: "loguniform"
      low: 1e-5
      high: 1e-2
    clip_param:
      type: "uniform"
      low: 0.1
      high: 0.3
    entropy_coef:
      type: "loguniform"
      low: 0.001
      high: 0.1
    gamma:
      type: "uniform"
      low: 0.99
      high: 0.999
    lambda:
      type: "uniform"
      low: 0.9
      high: 0.99

# Multi-task learning (if applicable)
multi_task:
  enabled: false
  tasks: ["locomotion", "navigation", "manipulation"]
  task_weights: [1.0, 0.5, 0.3]
  
# Distributed training
distributed:
  enabled: false
  num_workers: 4
  synchronous: true
  
# Mixed precision training
mixed_precision:
  enabled: true
  loss_scale: "dynamic"

# Model compression
compression:
  # Quantization
  quantization:
    enabled: false
    method: "dynamic"  # dynamic, static, qat
    
  # Pruning
  pruning:
    enabled: false
    sparsity: 0.1
    
  # Knowledge distillation
  distillation:
    enabled: false
    teacher_model_path: null
    temperature: 4.0
    alpha: 0.5

# Deployment optimization
deployment:
  # Model export
  export_format: "onnx"  # onnx, torchscript, tensorrt
  
  # Runtime optimization
  optimization_level: "O2"
  
  # Inference settings
  batch_size: 1
  max_sequence_length: 1
